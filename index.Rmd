---
title: "Prediction Assignment"
author: "C. Smith"
date: "March 31, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction 

The objective of this project is to take accelerometer data that monitors dumbbell motions and classify to determine if the exercise is being performed correctly.  The data is from http://groupware.les.inf.puc-rio.br/har. The data is from accelerometers on the belt, forearm, arm and dumbell of 6 participants. From their website:

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

I will attempt to predict the manner in which the participants did the exercise.

## Data

I downloaded the data and read it into a training and test set.


```{r echo = FALSE }
library(caret)
library(rattle)
library(rpart)
library(rpart.plot)
library(randomForest)

set.seed(123)
setwd("E://DataScience//Practical Machine Learning")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv")

trainingSet <- read.csv("pml-training.csv", header = T, na.strings = c("NA", ""))
testingSet <- read.csv("pml-testing.csv", header = T, na.strings = c("NA", ""))
dim(trainingSet)
dim(testingSet)
```

In the training set, there are 19,622 observations and 160 variables. The test set consists of 20 observations of 160 variables. The first seven variables don't look like they will be useful for prediction, e.g, X, user_name, etc., so I will remove those from both training and testing.
Of the remaining variables, many are NA, so I will remove those that have more than 80% the values as NA. 80% is arbitrary. That still leaves us with 53 variables. 
```{r }

unusedColumnNames <- colnames(trainingSet)[1:7]
unusedColumnNames
training <- trainingSet [,-which(names(trainingSet) %in% unusedColumnNames)]
testing <- testingSet [,-which(names(testingSet) %in% unusedColumnNames)]

tooMany <- 19622 *.8
training <- training[,colSums(is.na(training)) < tooMany]
tooMany <- 20 *.8
testing <- testing[,colSums(is.na(testing)) < tooMany]
dim(training)
dim(testing)

```

## Divide Training Data 
To estimate the prediction accuracy, I will divide the training set into two subsets, 70% into the first new training subset and 30% in the last training subset. These will be used as training and testing, instead of the test set.

```{r }
inTrain <- createDataPartition(y = training$classe, p = .7, list=F)
subTrain <- training[inTrain,]
subTest <-training[-inTrain,]
dim(subTrain)
dim(subTest)
```
## Build Model

The model I'll use is the random forest model.  I chose this model because the random forest algorithm works well with a large number of predictors (53) and calculates the strongest predictor at the top split and then the next and so on. The random forest algorithm will also give us the important variables. Random decision forests correct for decision trees' habit of overfitting to their training set, so this seems like a reasonable choice.

## Cross Validation

I'll use the 70% subset of the training  with Cross validation (cv) as the train control method and iterate 5 times to attempt to improve the prediction rate. Five is an arbitrary number.
```{r }
set.seed(123)
modCV <- train(subTrain$classe ~ .,  trControl=trainControl(method = "cv", number = 5), data = subTrain, method="rf")
prediction <- predict(modCV, newdata=subTest)
confusionMatrix(prediction, subTest$classe)

```

## Expected Out of Sample Error

The out of sample error is the rate at which the testing data is misclassified.  From the Confusion Matrix we can see that the misclassifications are:

A - 7
B - 10
C - 21
D - 4
E - 1

Which is a total of 43 misclassification. The test set has 5885 observations, so that gives an error rate of .0073.  This is also given by 1 - Accuracy, .0073

## Test Set Data

To predict the outcome with the original test set, we will feed the new data into the model.

```{r }

predict(modCV, newdata=testing)

```

## Conclusion

The random forest gave an excellent prediction with an accuracy of 99.27%. It also calculated the important variables, this is a plot of the top ten most important variables, as determined by the random forest model.  Five iterations may be excessive, however, as the cross validation model took over 10 minutes to build on my machine.  

```{r echo=FALSE}

plot(varImp(modCV, scale = FALSE), top = 10)

```


## Citation

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 

Wikipedia https://en.wikipedia.org/wiki/Random_forest

